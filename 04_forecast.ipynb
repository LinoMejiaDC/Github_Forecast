{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH WHERE THE FILES ARE SAVED\n",
    "path = './' #r'/dbfs/FileStore/tables/' # databricks filesystem\n",
    "azure_path = None #'/mnt/ddf/' # Azure Blob Storage\n",
    "\n",
    "# IMPORT OF DATABASE\n",
    "ddf = pd.read_excel(path + r'/2_cleaned_data/clean_ddf.xlsx')\n",
    "\n",
    "# IMPORT OF VARIABLES FROM MODELING NOTEBOOK\n",
    "modeling = (path + r'/4_outputs/var_results_modeling.sav')\n",
    "results = pickle.load(open(modeling, 'rb'))\n",
    "\n",
    "# IMPORT OF DATA FROM FRAMEWORK CONFIGURATION\n",
    "frameworkconfig1 = (path + r'/3_variables/var1_framework_config.sav')\n",
    "ddf_config_var = pickle.load(open(frameworkconfig1, 'rb'))\n",
    "\n",
    "# frameworkconfig2 = (path + r'/3_variables/var2_framework_config.sav')\n",
    "# ddf_config_kpi = pickle.load(open(frameworkconfig2, 'rb'))\n",
    "\n",
    "frameworkconfig3 = (path + r'/3_variables/var3_framework_config.sav')\n",
    "ddf_config_par = pickle.load(open(frameworkconfig3, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIMARY KEY\n",
    "primary_key = ddf_config_var.loc[np.where(ddf_config_var[\"VariableUsage\"]=='PRIMARY')[0][0],'VariableName']\n",
    "\n",
    "# FILTER BY COLUMN\n",
    "filter_by = ddf_config_par.loc[0,'Value']\n",
    "\n",
    "# FILTER BY VALUE\n",
    "filter_value = ddf_config_par.loc[1,'Value']\n",
    "\n",
    "if str(filter_by)!='nan':\n",
    "    # PRIMARY KEY LIST WITH FILTER \n",
    "    pk_list = ddf[primary_key][ddf[filter_by]==filter_value].unique()\n",
    "else:\n",
    "    # PRIMARY KEY LIST \n",
    "    pk_list = ddf[primary_key].unique()\n",
    "    \n",
    "# DATETIME VARIABLE\n",
    "date_column = ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0][0],'VariableName']\n",
    "\n",
    "# PERIODS TO PREDICT\n",
    "forecast = pd.to_numeric(ddf_config_par.loc[17,'Value'])\n",
    "\n",
    "# TARGET VARIABLE\n",
    "target = ddf_config_var.loc[np.where(ddf_config_var[\"VariableUsage\"]=='TARGET')[0][0],'VariableName']\n",
    "\n",
    "# EXOGENOUS VARIABLE\n",
    "exogenous = []\n",
    "for i in ddf_config_var.loc[np.where(np.logical_and(ddf_config_var[\"VariableUsage\"]=='EXOGENOUS',ddf_config_var[\"VariableType\"]=='NUMERIC'))[0],'VariableName']:\n",
    "    exogenous.append(i)\n",
    "for i in ddf_config_var.loc[np.where(np.logical_and(ddf_config_var[\"VariableUsage\"]=='EXOGENOUS',ddf_config_var[\"VariableType\"]=='CATEGORICAL'))[0],'VariableName']:\n",
    "    exogenous.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT OF DATETIME\n",
    "form = ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'Obs'][0]\n",
    "\n",
    "# DATETIME FREQUENCY\n",
    "if ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='days':\n",
    "    freq = 'D'\n",
    "elif ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='weeks':\n",
    "    freq = 'W'\n",
    "elif ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='months':\n",
    "    freq = 'M'\n",
    "elif ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='years':\n",
    "    freq = 'Y'\n",
    "\n",
    "# SET OF PRIMARY KEYS WITH COMPLETENESS\n",
    "ddf_completeness = ddf[[date_column,primary_key]]\n",
    "ddf_completeness[date_column] = pd.to_datetime(ddf_completeness[date_column], format=form)\n",
    "pk_completeness = ddf_completeness[[date_column,primary_key]].groupby(primary_key).agg(['min','max','count'])\n",
    "pk_completeness.columns = ['FirstDate','LastDate','Total']\n",
    "pk_completeness['ExpectedLength'] = pd.to_datetime(pk_completeness['LastDate'], format=form)-pd.to_datetime(pk_completeness['FirstDate'], format=form)\n",
    "pk_completeness['ExpectedLength'] = round(pk_completeness['ExpectedLength']/np.timedelta64(1,freq)+1,0)\n",
    "pk_completeness2 = set(pk_completeness.index[np.where(pk_completeness['Total']==pk_completeness['ExpectedLength'])])\n",
    "non_completeness = set(pk_list).difference(pk_completeness2)\n",
    "pk_list = np.array(list(set(pk_list).intersection(pk_completeness2)))\n",
    "pk_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort Values (must) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.sort_values([primary_key, date_column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SARIMAX MODEL\n",
    "\n",
    "parameters_sarimax_opt = list(results[[primary_key,'parameters_sarimax']].drop_duplicates()['parameters_sarimax'])\n",
    "product_sarimax_opt = list(results[[primary_key,'parameters_sarimax']].drop_duplicates()[primary_key])\n",
    "mape_sarimax_opt = list(results[[primary_key,'mape_sarimax']].drop_duplicates()['mape_sarimax'])\n",
    "\n",
    "\n",
    "product_sarimax = []\n",
    "forecast_sarimax = []\n",
    "mape_sarimax = []\n",
    "conf_int_sarimax = []\n",
    "\n",
    "for pro in pk_list:\n",
    "    p_params,d_params,q_params,P_params,D_params,Q_params,m_params,t_params = parameters_sarimax_opt[product_sarimax_opt.index(pro)]\n",
    "    # HISTORICAL DATA\n",
    "    data = ddf[target][ddf[primary_key]==pro]\n",
    "    # DATA OF EXOGENOUS VARIABLES\n",
    "    exog = ddf[exogenous][ddf[primary_key]==pro]\n",
    "    if exog.shape[1]==0:\n",
    "        model_sarimax_fit = SARIMAX(np.array(data),order=(p_params,d_params,q_params),\n",
    "                                    seasonal_order=(P_params,D_params,Q_params,m_params),\n",
    "                                    trend=t_params,enforce_stationarity=False,enforce_invertibility=False).fit()\n",
    "        product_sarimax.append(pro)\n",
    "        forecast_sarimax.append(model_sarimax_fit.get_forecast(steps=forecast).predicted_mean)\n",
    "        mape_sarimax.append(mape_sarimax_opt[product_sarimax_opt.index(pro)])\n",
    "        conf_int_sarimax.append(model_sarimax_fit.get_forecast(steps=forecast).conf_int())\n",
    "    else:\n",
    "        model_sarimax_fit = SARIMAX(np.array(data),exog=np.array(exog),order=(p_params,d_params,q_params),seasonal_order=(P_params,D_params,Q_params,m_params),trend=t_params,enforce_stationarity=False,enforce_invertibility=False).fit()\n",
    "        product_sarimax.append(pro)\n",
    "        exog_post = pd.DataFrame(index=range(forecast))\n",
    "        for i in range(0,exog.shape[1]):\n",
    "            if isinstance(exog.loc[exog.index[0],exogenous[i]], float)==True:\n",
    "                exog_post = pd.concat([exog_post, pd.DataFrame(np.repeat(np.matrix(np.mean(exog.iloc[:,i])),forecast,axis=0))], axis=1)\n",
    "            else:\n",
    "                exog_post = pd.concat([exog_post, pd.DataFrame(np.repeat(np.matrix(exog.iloc[:,i].mode()),forecast,axis=0))[0]], axis=1)\n",
    "        forecast_sarimax.append(model_sarimax_fit.get_forecast(steps=forecast, exog=exog_post).predicted_mean)\n",
    "        mape_sarimax.append(mape_sarimax_opt[product_sarimax_opt.index(pro)])\n",
    "        conf_int_sarimax.append(model_sarimax_fit.get_forecast(steps=forecast, exog=exog_post).conf_int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL ETS MODEL\n",
    "parameters_ets_opt = list(results[[primary_key,'parameters_ets']].drop_duplicates()['parameters_ets'])\n",
    "product_ets_opt = list(results[[primary_key,'parameters_ets']].drop_duplicates()[primary_key])\n",
    "mape_ets_opt = list(results[[primary_key,'mape_ets']].drop_duplicates()['mape_ets'])\n",
    "\n",
    "product_es = []\n",
    "forecast_es = []\n",
    "mape_es = []\n",
    "\n",
    "for pro in pk_list:\n",
    "    t_params,d_params,s_params,p_params,b_params,r_params = parameters_ets_opt[product_ets_opt.index(pro)]\n",
    "    # HISTORICAL DATA\n",
    "    data = ddf[target][ddf[primary_key]==pro]\n",
    "    model_es_fit = ExponentialSmoothing(np.array(data),trend=t_params,damped=d_params,seasonal=s_params,seasonal_periods=p_params).fit(optimized=True, use_boxcox=b_params, remove_bias=r_params)\n",
    "    product_es.append(pro)\n",
    "    forecast_es.append(model_es_fit.forecast(steps=forecast))\n",
    "    mape_es.append(mape_ets_opt[product_ets_opt.index(pro)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAFRAME SUMMARY (FORECAST)\n",
    "predict_sarimax_final = pd.DataFrame(product_sarimax)\n",
    "predict_sarimax_final.columns = [primary_key]\n",
    "predict_sarimax_final['predict_sarimax'] = forecast_sarimax\n",
    "predict_sarimax_final['mape_sarimax'] = mape_sarimax\n",
    "\n",
    "# predict_sarimax_final = predict_sarimax_final.predict_sarimax.apply(pd.Series)\\\n",
    "# .merge(predict_sarimax_final, left_index = True, right_index = True)\\\n",
    "# .drop ([\"predict_sarimax\"], axis = 1)\\\n",
    "# .melt (id_vars = [primary_key,'mape_sarimax'], value_name = \"predict_sarimax\")\\\n",
    "# .sort_values(by=[primary_key,'variable'])\\\n",
    "# .dropna()\n",
    "\n",
    "predict_sarimax_final = pd.melt(predict_sarimax_final.predict_sarimax.apply(pd.Series).merge(predict_sarimax_final, left_index = True, right_index = True)\\\n",
    ".drop (['predict_sarimax'], axis = 1),id_vars=[primary_key,'mape_sarimax'], value_vars=list(range(0,forecast)))\\\n",
    ".sort_values(by=[primary_key,'variable'])\\\n",
    ".dropna()\n",
    "\n",
    "conf_int = pd.DataFrame()\n",
    "for i in range(0,len(pk_list)):\n",
    "  conf_int = conf_int.append(pd.DataFrame(conf_int_sarimax[i]))\n",
    "conf_int.columns = ['2.5%','97.5%']\n",
    "\n",
    "predict_sarimax_final = predict_sarimax_final.reset_index(drop=True)\n",
    "conf_int = conf_int.reset_index(drop=True)\n",
    "\n",
    "predict_sarimax_final = predict_sarimax_final.join(conf_int)\n",
    "\n",
    "# DATAFRAME SUMMARY (FORECAST)\n",
    "predict_es_final = pd.DataFrame(product_es)\n",
    "predict_es_final.columns = [primary_key]\n",
    "predict_es_final['predict_es'] = forecast_es\n",
    "predict_es_final['mape_es'] = mape_es\n",
    "\n",
    "# predict_es_final = predict_es_final.predict_es.apply(pd.Series)\\\n",
    "# .merge(predict_es_final, left_index = True, right_index = True)\\\n",
    "# .drop ([\"predict_es\"], axis = 1)\\\n",
    "# .melt (id_vars = [primary_key,'mape_es'], value_name = \"predict_es\")\\\n",
    "# .sort_values(by=[primary_key,'variable'])\\\n",
    "# .dropna()\\\n",
    "# .drop([primary_key], axis = 1)\n",
    "\n",
    "predict_es_final = pd.melt(predict_es_final.predict_es.apply(pd.Series).merge(predict_es_final, left_index = True, right_index = True).drop ([\"predict_es\"], axis = 1),id_vars=[primary_key,'mape_es'], value_vars=list(range(0,forecast))).sort_values(by=[primary_key,'variable']).dropna()\n",
    "\n",
    "# JOIN OF DATAFRAMES\n",
    "merged_results = pd.merge(predict_sarimax_final, predict_es_final,  how='left', left_on=[primary_key,'variable'], right_on = [primary_key,'variable'])\n",
    "merged_results = merged_results.rename(columns={'value_x': 'sarimax_forecast', 'value_y': 'ets_forecast'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCEL WITH THE DATA OF MODELS\n",
    "writer = pd.ExcelWriter(path + r'/4_outputs/ddf_forecast.xlsx')\n",
    "merged_results.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if azure_path is not None:\n",
    "    dbutils.fs.cp(path.replace('/dbfs/','dbfs:/') + \"/ddf_forecast.xlsx\", azure_path + '/4_outputs/ddf_forecast.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "04_forecast",
  "notebookId": 2506651724598203
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
