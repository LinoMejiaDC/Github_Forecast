{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from itertools import product\n",
    "from joblib import Parallel\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import delayed\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# from arch.unitroot import PhillipsPerron\n",
    "\n",
    "\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH WHERE THE FILES ARE SAVED\n",
    "azure_path = None #'/mnt/ddf/' # Azure Blob Storage\n",
    "path = r'./'\n",
    "\n",
    "# IMPORT OF DATABASE\n",
    "ddf = pd.read_excel(path + r'/2_cleaned_data/clean_ddf.xlsx')\n",
    "# ddf = pd.read_csv(path + r'clean_ddf.csv',sep=',')\n",
    "\n",
    "# IMPORT OF DATA FROM FRAMEWORK CONFIGURATION\n",
    "frameworkconfig1 = (path + r'/3_variables/var1_framework_config.sav')\n",
    "ddf_config_var = pickle.load(open(frameworkconfig1, 'rb'))\n",
    "\n",
    "# frameworkconfig2 = (path + r'/3_variables/var2_framework_config.sav')\n",
    "# ddf_config_kpi = pickle.load(open(frameworkconfig2, 'rb'))\n",
    "\n",
    "frameworkconfig3 = (path + r'/3_variables/var3_framework_config.sav')\n",
    "ddf_config_par = pickle.load(open(frameworkconfig3, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIMARY KEY\n",
    "primary_key = ddf_config_var.loc[np.where(ddf_config_var[\"VariableUsage\"]=='PRIMARY')[0][0],'VariableName']\n",
    "\n",
    "# FILTER BY COLUMN\n",
    "filter_by = ddf_config_par.loc[0,'Value']\n",
    "\n",
    "# FILTER BY VALUE\n",
    "filter_value = ddf_config_par.loc[1,'Value']\n",
    "\n",
    "if str(filter_by)!='nan':\n",
    "    # PRIMARY KEY LIST WITH FILTER \n",
    "    pk_list = ddf[primary_key][ddf[filter_by]==filter_value].unique()\n",
    "else:\n",
    "    # PRIMARY KEY LIST \n",
    "    pk_list = ddf[primary_key].unique()\n",
    "    \n",
    "# DATETIME VARIABLE\n",
    "date_column = ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0][0],'VariableName']\n",
    "\n",
    "# TARGET VARIABLE\n",
    "target = ddf_config_var.loc[np.where(ddf_config_var[\"VariableUsage\"]=='TARGET')[0][0],'VariableName']\n",
    "\n",
    "# EXOGENOUS VARIABLE\n",
    "exogenous = []\n",
    "for i in ddf_config_var.loc[np.where(np.logical_and(ddf_config_var[\"VariableUsage\"]=='EXOGENOUS',ddf_config_var[\"VariableType\"]=='NUMERIC'))[0],'VariableName']:\n",
    "    exogenous.append(i)\n",
    "for i in ddf_config_var.loc[np.where(np.logical_and(ddf_config_var[\"VariableUsage\"]=='EXOGENOUS',ddf_config_var[\"VariableType\"]=='CATEGORICAL'))[0],'VariableName']:\n",
    "    exogenous.append(i)\n",
    "\n",
    "# TRAINING PERCENTAGE\n",
    "ptrain = pd.to_numeric(ddf_config_par.loc[2,'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no series with problems of completeness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ey538cs\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# FORMAT OF DATETIME\n",
    "form = ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'Obs'][0]\n",
    "\n",
    "# DATETIME FREQUENCY\n",
    "if ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='days':\n",
    "    freq = 'D'\n",
    "elif ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='weeks':\n",
    "    freq = 'W'\n",
    "elif ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='months':\n",
    "    freq = 'M'\n",
    "elif ddf_config_var.loc[np.where(ddf_config_var[\"VariableType\"]=='DATETIME')[0],'UpdateFrequency'][0]=='years':\n",
    "    freq = 'Y'\n",
    "\n",
    "# SET OF PRIMARY KEYS WITH COMPLETENESS\n",
    "ddf_completeness = ddf[[date_column,primary_key]]\n",
    "ddf_completeness[date_column] = pd.to_datetime(ddf_completeness[date_column], format=form)\n",
    "pk_completeness = ddf_completeness[[date_column,primary_key]].groupby(primary_key).agg(['min','max','count'])\n",
    "pk_completeness.columns = ['FirstDate','LastDate','Total']\n",
    "pk_completeness['ExpectedLength'] = pd.to_datetime(pk_completeness['LastDate'], format=form)-pd.to_datetime(pk_completeness['FirstDate'], format=form)\n",
    "pk_completeness['ExpectedLength'] = round(pk_completeness['ExpectedLength']/np.timedelta64(1,freq)+1,0)\n",
    "pk_completeness2 = set(pk_completeness.index[np.where(pk_completeness['Total']==pk_completeness['ExpectedLength'])])\n",
    "non_completeness = set(pk_list).difference(pk_completeness2)\n",
    "pk_list = np.array(list(set(pk_list).intersection(pk_completeness2)))\n",
    "pk_list.sort()\n",
    "\n",
    "if len(non_completeness)==0:\n",
    "    print('There are no series with problems of completeness')\n",
    "else:\n",
    "    print('The following primary keys have completeness problems:', non_completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort Values (must) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.sort_values([primary_key, date_column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-STEP SARIMAX FORECAST\n",
    "def sarima_forecast(history, exog, config, forecast):\n",
    "    p,d,q,P,D,Q,m,t = config\n",
    "    if exog.shape[1]==0:\n",
    "        # define model\n",
    "        try:\n",
    "            model = SARIMAX(endog=np.array(history), order=(p,d,q), seasonal_order=(P,D,Q,m), trend=t, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            # fit model\n",
    "            model_fit = model.fit(disp=False)\n",
    "            # make one step forecast\n",
    "            yhat = model_fit.forecast(steps=forecast)\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "    else:\n",
    "        # define model\n",
    "        model = SARIMAX(endog=np.array(history), exog=np.array(exog), order=(p,d,q), seasonal_order=(P,D,Q,m), trend=t, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        # fit model\n",
    "        model_fit = model.fit(disp=False)\n",
    "        exog_post = pd.DataFrame(index=range(forecast))\n",
    "        for i in range(0,exog.shape[1]):\n",
    "            if isinstance(exog.loc[exog.index[0],exogenous[i]], float)==True:\n",
    "                exog_post = pd.concat([exog_post, pd.DataFrame(np.repeat(np.matrix(np.mean(exog.iloc[:,i])),forecast,axis=0))], axis=1)\n",
    "            else:\n",
    "                exog_post = pd.concat([exog_post, pd.DataFrame(np.repeat(np.matrix(exog.iloc[:,i].mode()),forecast,axis=0))[0]], axis=1)\n",
    "        # make multi-step forecast\n",
    "        yhat = model_fit.forecast(steps=forecast, exog=exog_post)\n",
    "    return model_fit, yhat\n",
    "\n",
    "# SPLIT A DATASET INTO TRAIN/TEST SETS\n",
    "def train_test_split(data, p_train):\n",
    "    return np.asarray(data)[range(0,int(round(p_train*len(data),0))+1)], np.asarray(data)[range(int(round(p_train*len(data),0)),len(data))]\n",
    "\n",
    "\n",
    "# MAPE ERROR CALCULATION\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true[y_true==0] = epsilon\n",
    "    return round(np.mean(np.abs((np.array(y_true)-np.array(y_pred))/np.array(y_true)))*100,2)\n",
    "\n",
    "# WALK-FORWARD VALIDATION\n",
    "def walk_forward_validation_sarimax(data, p_train, cfg):\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, p_train)\n",
    "    # fit model and make forecast for history\n",
    "    predictions = sarima_forecast(train, exog[0:len(train)], cfg, len(test))[1]\n",
    "    # estimate prediction error\n",
    "    error = mean_absolute_percentage_error(test, predictions)\n",
    "    return error\n",
    "\n",
    "# SCORE A MODEL\n",
    "def score_model_sarimax(data, p_train, cfg, debug=False):\n",
    "    # show all warnings and fail on exception if debugging\n",
    "    if debug:\n",
    "        result = walk_forward_validation_sarimax(data, p_train, cfg)\n",
    "    else:\n",
    "        # one failure during model validation suggests an unstable config\n",
    "        try:\n",
    "            # never show warnings when grid searching, too noisy\n",
    "            with catch_warnings():\n",
    "                filterwarnings(\"ignore\")\n",
    "                result = walk_forward_validation_sarimax(data, p_train, cfg)\n",
    "        except Exception as err:\n",
    "            result = None\n",
    "    # check for an interesting result\n",
    "    if result is not None:\n",
    "        print('Model[%s] -> %.5f' % (cfg, result))\n",
    "    return (cfg, result)\n",
    "\n",
    "\n",
    "# GRID SEARCH CONFIGS\n",
    "def grid_search(data, cfg_list, p_train, score_model, parallel=False):\n",
    "    scores = None\n",
    "    if parallel:\n",
    "        # execute configs in parallel\n",
    "        executor = Parallel(n_jobs=cpu_count(), verbose = 10)\n",
    "        tasks = (delayed(score_model)(data, p_train, cfg) for cfg in cfg_list)\n",
    "        scores = executor(tasks)\n",
    "    else:\n",
    "        scores = [score_model(data, p_train, cfg) for cfg in cfg_list]\n",
    "    # remove empty results\n",
    "    scores = [r for r in scores if r[1] != None and str(r[1]) != 'nan']\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores\n",
    "\n",
    "\n",
    "# CREATE SET OF SARIMAX CONFIGS TO TRY\n",
    "def sarima_configs():\n",
    "    # define config lists\n",
    "    if isinstance(ddf_config_par.loc[3,'Value'], int):\n",
    "        p_params = np.arange(pd.to_numeric(ddf_config_par.loc[3,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[3,'Value']), list):\n",
    "        p_params = ast.literal_eval(ddf_config_par.loc[3,'Value']).tolist()\n",
    "    if isinstance(ddf_config_par.loc[4,'Value'], int):\n",
    "        d_params = np.arange(pd.to_numeric(ddf_config_par.loc[4,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[4,'Value']), list):\n",
    "        d_params = ast.literal_eval(ddf_config_par.loc[4,'Value']).tolist()\n",
    "    if isinstance(ddf_config_par.loc[5,'Value'], int):\n",
    "        q_params = np.arange(pd.to_numeric(ddf_config_par.loc[5,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[5,'Value']), list):\n",
    "        q_params = ast.literal_eval(ddf_config_par.loc[5,'Value']).tolist()\n",
    "    if isinstance(ddf_config_par.loc[6,'Value'], int):\n",
    "        P_params = np.arange(pd.to_numeric(ddf_config_par.loc[6,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[6,'Value']), list):\n",
    "        P_params = ast.literal_eval(ddf_config_par.loc[6,'Value']).tolist()\n",
    "    if isinstance(ddf_config_par.loc[7,'Value'], int):\n",
    "        D_params = np.arange(pd.to_numeric(ddf_config_par.loc[7,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[7,'Value']), list):\n",
    "        D_params = ast.literal_eval(ddf_config_par.loc[7,'Value']).tolist()\n",
    "    if isinstance(ddf_config_par.loc[8,'Value'], int):\n",
    "        Q_params = np.arange(pd.to_numeric(ddf_config_par.loc[8,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[8,'Value']), list):\n",
    "        Q_params = ast.literal_eval(ddf_config_par.loc[8,'Value']).tolist()\n",
    "    if isinstance(ddf_config_par.loc[9,'Value'], int):\n",
    "        m_params = np.arange(pd.to_numeric(ddf_config_par.loc[9,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[9,'Value']), list):\n",
    "        m_params = ast.literal_eval(ddf_config_par.loc[9,'Value'])\n",
    "    t_params = ast.literal_eval(ddf_config_par.loc[10,'Value'])\n",
    "    # create config instances\n",
    "    models = set(list(product(p_params,d_params,q_params,P_params,D_params,Q_params,m_params,t_params)))\n",
    "    return models\n",
    "\n",
    "# DATE OF TEST\n",
    "def date_test(prod, p_train):\n",
    "    result = tuple(np.array(ddf[date_column][ddf[primary_key]==prod])[range(int(round(p_train*len(ddf[date_column][ddf[primary_key]==prod]),0)),len(ddf[date_column][ddf[primary_key]==prod]))])\n",
    "    return result\n",
    "    \n",
    "# FORECAST OF TEST\n",
    "def forecast_test(data, p_train):\n",
    "    result = tuple(sarima_forecast(train_test_split(data, p_train)[0], exog[0:len(train_test_split(data, p_train)[0])],scores[0][0], len(train_test_split(data, p_train)[1]))[1])\n",
    "    return result\n",
    "\n",
    "# HETEROSCEDASTICITY TEST\n",
    "def test_heteroscedasticity(data,p_train):\n",
    "    model_fit = sarima_forecast(train_test_split(data, p_train)[0], exog[0:len(train_test_split(data, p_train)[0])], scores[0][0], 1)[0]\n",
    "    if model_fit.test_heteroskedasticity(None)[0][1]<0.05:\n",
    "        result = 'heteroscedastic'\n",
    "    else:\n",
    "        result = 'homoscedastic'\n",
    "    return result\n",
    "\n",
    "# AUGMENTED DICKEY-FULLER TEST (HIPÓTESIS NULA: SERIE NO ESTACIONARIA)\n",
    "def dickey_fuller(data,p_train):\n",
    "    if adfuller(train_test_split(data, p_train)[0])[1]<0.05:\n",
    "        result = 'stationary'\n",
    "    else:\n",
    "        result = 'non-stationary'\n",
    "    return result\n",
    "\n",
    "# PHILLIPS-PERRON TEST\n",
    "def phillips_perron(data,p_train):\n",
    "    try:\n",
    "        if PhillipsPerron(train_test_split(data, p_train)[0]).pvalue<0.05:\n",
    "            result = 'stationary'\n",
    "        else:\n",
    "            result = 'non-stationary'\n",
    "    except:\n",
    "        result = 'not-applicable'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    6.8s remaining:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    6.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0862s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0650s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1434s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1092s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1366s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0830s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1084s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1100s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1248s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1128s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1096s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1414s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1258s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1050s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail7\n",
      "finished primary key Retail8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATIONS TO TEST\n",
    "cfg_list = sarima_configs()\n",
    "# cfg_list = set(list(cfg_list)[:4])\n",
    "sarimax_grid_search = list()\n",
    "\n",
    "for pro in pk_list:\n",
    "    # HISTORICAL DATA\n",
    "    data = ddf[target][ddf[primary_key]==pro]\n",
    "    # EXOGENOUS VARIABLE DATA\n",
    "    exog = ddf[exogenous][ddf[primary_key]==pro]\n",
    "    # GRID SEARCH\n",
    "    scores = grid_search(data, cfg_list, ptrain, score_model_sarimax, True)\n",
    "    print('finished primary key',pro)\n",
    "    # SAVE TOP CONFIG BY PK\n",
    "    # sarimax_grid_search.append((pro,)+scores[0]+(date_test(pro,ptrain),)+(forecast_test(data,ptrain),)+(test_heteroscedasticity(data,ptrain),)+(dickey_fuller(data,ptrain),)+(phillips_perron(data,ptrain),))\n",
    "    sarimax_grid_search.append((pro,)+scores[0]+(date_test(pro,ptrain),)+(forecast_test(data,ptrain),)+(test_heteroscedasticity(data,ptrain),)+(dickey_fuller(data,ptrain),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-STEP HOLT WINTER'S EXPONENTIAL SMOOTHING FORECAST\n",
    "def ets_forecast(history, config, forecast):\n",
    "\tt,d,s,p,b,r = config\n",
    "\t# define model model\n",
    "\tmodel = ExponentialSmoothing(np.array(history), trend=t, damped=d, seasonal=s, seasonal_periods=p)\n",
    "\t# fit model\n",
    "\tmodel_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n",
    "\t# make multi step forecast\n",
    "\tyhat = model_fit.forecast(steps=forecast)\n",
    "\treturn yhat\n",
    "\n",
    "# WALK-FORWARD VALIDATION\n",
    "def walk_forward_validation_ets(data, p_train, cfg):\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, p_train)\n",
    "    # fit model and make forecast for history\n",
    "\tpredictions = ets_forecast(train, cfg, len(test))\n",
    "\t# estimate prediction error\n",
    "\terror = mean_absolute_percentage_error(test, predictions)\n",
    "\treturn error\n",
    "\n",
    "# SCORE A MODEL\n",
    "def score_model_ets(data, p_train, cfg, debug=False):\n",
    "\t# show all warnings and fail on exception if debugging\n",
    "\tif debug:\n",
    "\t\tresult = walk_forward_validation_ets(data, p_train, cfg)\n",
    "\telse:\n",
    "\t\t# one failure during model validation suggests an unstable config\n",
    "\t\ttry:\n",
    "\t\t\t# never show warnings when grid searching, too noisy\n",
    "\t\t\twith catch_warnings():\n",
    "\t\t\t\tfilterwarnings(\"ignore\")\n",
    "\t\t\t\tresult = walk_forward_validation_ets(data, p_train, cfg)\n",
    "\t\texcept:\n",
    "\t\t\tresult = None\n",
    "\t# check for an interesting result\n",
    "\tif result is not None:\n",
    "\t\tprint('Model[%s] -> %.5f' % (cfg, result))\n",
    "\treturn (cfg, result)\n",
    "\n",
    "\n",
    "# CREATE SET OF SARIMA CONFIGS TO TRY\n",
    "def ets_configs():\n",
    "    # define config lists\n",
    "    t_params = ast.literal_eval(ddf_config_par.loc[11,'Value'])\n",
    "    d_params = ast.literal_eval(ddf_config_par.loc[12,'Value'])\n",
    "    s_params = ast.literal_eval(ddf_config_par.loc[13,'Value'])\n",
    "    if isinstance(ddf_config_par.loc[14,'Value'], int):\n",
    "        p_params = np.arange(pd.to_numeric(ddf_config_par.loc[14,'Value'])+1)\n",
    "    elif isinstance(ast.literal_eval(ddf_config_par.loc[14,'Value']), list):\n",
    "        p_params = ast.literal_eval(ddf_config_par.loc[14,'Value'])\n",
    "    b_params = ast.literal_eval(ddf_config_par.loc[15,'Value'])\n",
    "    r_params = ast.literal_eval(ddf_config_par.loc[16,'Value'])\n",
    "    # create config instances\n",
    "    models = set(list(product(t_params,d_params,s_params,p_params,b_params,r_params)))\n",
    "    return models\n",
    "\n",
    "# FORECAST OF TEST\n",
    "def forecast_test(data, p_train):\n",
    "    result = tuple(ets_forecast(train_test_split(data, p_train)[0], scores[0][0], len(train_test_split(data, p_train)[1])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0970s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   10.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0810s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1092s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1248s.) Setting batch_size=2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1092s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0312s.) Setting batch_size=12.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0780s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0624s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0834s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1092s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0780s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0680s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0936s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1092s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished primary key Retail8\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATIONS TO TEST\n",
    "cfg_list = ets_configs()\n",
    "# cfg_list = set(list(ets_configs())[:5])\n",
    "ets_grid_search = list()\n",
    "\n",
    "for pro in pk_list:\n",
    "    # HISTORICAL DATA\n",
    "    data = ddf[target][ddf[primary_key]==pro]\n",
    "    # GRID SEARCH\n",
    "    scores = grid_search(data, cfg_list, ptrain, score_model_ets, True)\n",
    "    print('finished primary key',pro)\n",
    "    # SAVE TOP CONFIG BY PK\n",
    "    ets_grid_search.append((pro,)+scores[0]+(date_test(pro,ptrain),)+(forecast_test(data,ptrain),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY OF DATAFRAME (SARIMAX)\n",
    "sarimax_results = pd.DataFrame(sarimax_grid_search)\n",
    "# sarimax_results.columns = [primary_key,'parameters_sarimax','mape_sarimax',date_column,'forecast_sarimax','heteroscedasticity test','dickey-fuller test','phillip-perron test']\n",
    "sarimax_results.columns = [primary_key,'parameters_sarimax','mape_sarimax',date_column,'forecast_sarimax','heteroscedasticity test','dickey-fuller test']\n",
    "\n",
    "# SUMMARY OF DATAFRAME (ETS)\n",
    "ets_results = pd.DataFrame(ets_grid_search)\n",
    "ets_results.columns = [primary_key,'parameters_ets','mape_ets',date_column,'forecast_ets']\n",
    "\n",
    "# JOIN OF DATAFRAMES\n",
    "merged_results = pd.merge(sarimax_results, ets_results, how='left', left_on = [primary_key,date_column], right_on = [primary_key,date_column])\n",
    "# merged_results = sarimax_results\n",
    "\n",
    "# FINAL DATAFRAME\n",
    "\n",
    "prod = []\n",
    "par_sar = []\n",
    "sar_mape = []\n",
    "for_sar = []\n",
    "par_es = []\n",
    "es_mape = []\n",
    "for_es = []\n",
    "date = []\n",
    "het_test = []\n",
    "dickey_test =[]\n",
    "# phil_test = []\n",
    "\n",
    "cont = 0\n",
    "for i in merged_results['forecast_sarimax']:\n",
    "    array = np.array(i)\n",
    "    for j in array:\n",
    "        prod.append(merged_results[primary_key][cont])\n",
    "        par_sar.append(merged_results['parameters_sarimax'][cont])\n",
    "        sar_mape.append(merged_results['mape_sarimax'][cont])\n",
    "        es_mape.append(merged_results['mape_ets'][cont])\n",
    "        par_es.append(merged_results['parameters_ets'][cont])\n",
    "        het_test.append(merged_results['heteroscedasticity test'][cont])\n",
    "        dickey_test.append(merged_results['dickey-fuller test'][cont])\n",
    "        # phil_test.append(merged_results['phillip-perron test'][cont])\n",
    "      \n",
    "        for_sar.append(j)\n",
    "    cont +=1    \n",
    "\n",
    "for i in merged_results[date_column]:\n",
    "    array = np.array(i)\n",
    "    for j in array:\n",
    "        date.append(j)\n",
    "        \n",
    "for i in merged_results['forecast_ets']:\n",
    "    array = np.array(i)\n",
    "    for j in array:\n",
    "        for_es.append(j) \n",
    "        \n",
    "results = pd.DataFrame(prod)\n",
    "results.columns = [primary_key]\n",
    "results[date_column] = date\n",
    "results['parameters_sarimax'] = par_sar\n",
    "results['mape_sarimax'] = sar_mape\n",
    "results['heteroscedasticity test'] = het_test\n",
    "# results['phillip-perron test'] = phil_test\n",
    "results['dickey-fuller test'] = dickey_test\n",
    "results['forecast_sarimax'] = for_sar\n",
    "results['parameters_ets'] = par_es\n",
    "results['mape_ets'] = es_mape\n",
    "results['forecast_ets'] = for_es\n",
    "\n",
    "# MERGE PREDCTIONS RESULT WITH DDF DATAFRAME\n",
    "result_final = pd.merge(ddf, results, how='left', left_on = [primary_key,date_column], right_on = [primary_key,date_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCEL WITH DATA OF MODELS\n",
    "writer = pd.ExcelWriter(path + r'/4_outputs/ddf_predictions.xlsx')\n",
    "result_final.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "# OUTPUT MODEL   \n",
    "modeling = (path + r'/4_outputs/var_results_modeling.sav')\n",
    "pickle.dump(results, open(modeling, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if azure_path is not None:\n",
    "    dbutils.fs.cp(path.replace('/dbfs/','dbfs:/') + \"/ddf_predictions.xlsx\", azure_path + '/4_outputs/ddf_predictions.xlsx')\n",
    "    dbutils.fs.cp(path.replace('/dbfs/','dbfs:/') + \"/var_results_modeling.sav\", azure_path + '/3_variables/var_results_modeling.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "03_modeling",
  "notebookId": 2639857031304143
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
